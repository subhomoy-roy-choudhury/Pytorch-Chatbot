{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yaml2json.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "final=str(thisdir)+'\\yamls'\n",
    "arr = os.listdir(final)\n",
    "\n",
    "for i in arr:\n",
    "    a = os.path.join(final, i)\n",
    "    print(a)\n",
    "    if os.path.exists(a):\n",
    "        source_file = open(a, \"r\")\n",
    "        source_content = yaml.safe_load(source_file)\n",
    "        source_file.close()\n",
    "    else:\n",
    "        print(\"ERROR: \" + str(i)+ \" not found\")\n",
    "        exit(1)\n",
    "\n",
    "    # Processing the conversion\n",
    "    output = json.dumps(source_content,ensure_ascii=False, indent=2)\n",
    "    b='jsons/json_normal/'+str(i) + '.json'\n",
    "    target_file = open(b, \"w\")\n",
    "    target_file.write(output)\n",
    "    target_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "final=str(thisdir)+'\\jsons\\json_normal'\n",
    "arr = os.listdir(final)\n",
    "\n",
    "for i in arr:\n",
    "    a = os.path.join(final, i)\n",
    "    print(a)\n",
    "    with open(a, 'r') as f:\n",
    "        intents = json.load(f)\n",
    "\n",
    "    dict1={}\n",
    "    dict2={}\n",
    "    list1 = []\n",
    "    pattern_list = []\n",
    "    responses_list = []\n",
    "    i=0\n",
    "    for key,value in intents.items():\n",
    "        if key == 'conversations':\n",
    "            for i in range((len(intents['conversations']))):\n",
    "                pattern_list = []\n",
    "                responses_list = []\n",
    "                tag = str(str(intents['categories'][0])) +\" \"+str(i)\n",
    "                patterns = (intents['conversations'][i][:1])\n",
    "                responses = (intents['conversations'][i][1:])\n",
    "                print(patterns,tag,responses,i)\n",
    "                pattern_list.append(patterns)\n",
    "                responses_list.append(responses)\n",
    "                dict2 = {\"tag\":tag,\"patterns\":patterns,\"responses\":responses}\n",
    "                list1.append(dict2)\n",
    "                print(list1)\n",
    "        if key == 'categories':\n",
    "\n",
    "            key = str(intents['categories'][0])\n",
    "            a='jsons/json_convert/'+str(key) + '.json'\n",
    "            # dict1.__setitem__(str(key),list1)\n",
    "            dict1.__setitem__(\"intents\",list1)\n",
    "    print(intents.keys())\n",
    "\n",
    "\n",
    "\n",
    "    with open(a, 'w') as outfile:\n",
    "        json.dump(dict1, outfile,ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "final=str(thisdir)+'\\jsons\\json_convert'\n",
    "arr = os.listdir(final)\n",
    "\n",
    "# python merge.py intents_new.json\n",
    "tags=[]\n",
    "patterns=[]\n",
    "responses=[]\n",
    "\n",
    "with open('jsons/master_intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "\n",
    "# a=sys.argv[1]\n",
    "for i in arr:\n",
    "    a = os.path.join(final, i)\n",
    "    print(a)\n",
    "    with open(a, 'r') as f:\n",
    "        intents1 = json.load(f)\n",
    "\n",
    "    for i in intents['intents']:\n",
    "        tags.append(i['tag'])\n",
    "        patterns.append(i['patterns'])\n",
    "        responses.append(i['responses'])\n",
    "\n",
    "    for i in intents1['intents']:\n",
    "        tags_x=i['tag']\n",
    "        if tags_x in tags:\n",
    "            x=tags.index(tags_x)\n",
    "            y=intents['intents'][x]\n",
    "            for j in i['patterns']:\n",
    "                if j not in patterns[x]:\n",
    "                    y['patterns'].append(j)\n",
    "            for k in i['responses']:\n",
    "                if k not in responses[x]:\n",
    "                    y['responses'].append(k)\n",
    "        else:\n",
    "            dict1={'tag': i['tag'],\n",
    "                'patterns': i['patterns'],\n",
    "                'responses': i['responses'],\n",
    "            }\n",
    "            intents['intents'].append(dict1)\n",
    "            \n",
    "\n",
    "# for i in tqdm(range(len(intents['intents']))):\n",
    "#     sleep(1)\n",
    "\n",
    "with open('jsons/sample_intents.json', 'w') as outfile:\n",
    "    json.dump(intents, outfile,ensure_ascii=False, indent=4)\n",
    "\n",
    "# os.remove(a)\n",
    "# print(\"File Removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 patterns\n",
      "427 tags: ['AI 0', 'AI 1', 'AI 10', 'AI 11', 'AI 12', 'AI 13', 'AI 14', 'AI 15', 'AI 16', 'AI 17', 'AI 18', 'AI 19', 'AI 2', 'AI 20', 'AI 21', 'AI 22', 'AI 23', 'AI 24', 'AI 25', 'AI 26', 'AI 27', 'AI 28', 'AI 29', 'AI 3', 'AI 30', 'AI 31', 'AI 32', 'AI 33', 'AI 34', 'AI 35', 'AI 36', 'AI 37', 'AI 38', 'AI 39', 'AI 4', 'AI 40', 'AI 41', 'AI 42', 'AI 43', 'AI 44', 'AI 45', 'AI 46', 'AI 47', 'AI 48', 'AI 49', 'AI 5', 'AI 50', 'AI 51', 'AI 52', 'AI 53', 'AI 54', 'AI 55', 'AI 56', 'AI 57', 'AI 58', 'AI 59', 'AI 6', 'AI 7', 'AI 8', 'AI 9', 'computers 0', 'computers 1', 'computers 2', 'computers 3', 'computers 4', 'computers 5', 'computers 6', 'computers 7', 'computers 8', 'computers 9', 'conversations 0', 'conversations 1', 'conversations 10', 'conversations 11', 'conversations 12', 'conversations 13', 'conversations 14', 'conversations 15', 'conversations 16', 'conversations 17', 'conversations 18', 'conversations 19', 'conversations 2', 'conversations 20', 'conversations 21', 'conversations 22', 'conversations 3', 'conversations 4', 'conversations 5', 'conversations 6', 'conversations 7', 'conversations 8', 'conversations 9', 'delivery', 'emotion 0', 'emotion 1', 'emotion 10', 'emotion 11', 'emotion 12', 'emotion 13', 'emotion 14', 'emotion 15', 'emotion 16', 'emotion 17', 'emotion 18', 'emotion 19', 'emotion 2', 'emotion 20', 'emotion 21', 'emotion 22', 'emotion 23', 'emotion 24', 'emotion 25', 'emotion 26', 'emotion 27', 'emotion 28', 'emotion 29', 'emotion 3', 'emotion 30', 'emotion 31', 'emotion 32', 'emotion 33', 'emotion 34', 'emotion 35', 'emotion 36', 'emotion 37', 'emotion 38', 'emotion 39', 'emotion 4', 'emotion 40', 'emotion 41', 'emotion 42', 'emotion 43', 'emotion 44', 'emotion 45', 'emotion 46', 'emotion 47', 'emotion 5', 'emotion 6', 'emotion 7', 'emotion 8', 'emotion 9', 'food 0', 'food 1', 'food 10', 'food 11', 'food 12', 'food 2', 'food 3', 'food 4', 'food 5', 'food 6', 'food 7', 'food 8', 'food 9', 'funny', 'goodbye', 'gossip 0', 'gossip 1', 'gossip 2', 'gossip 3', 'gossip 4', 'gossip 5', 'greeting', 'greetings 0', 'greetings 1', 'greetings 2', 'greetings 3', 'greetings 4', 'greetings 5', 'greetings 6', 'greetings 7', 'greetings 8', 'health 0', 'history 0', 'history 1', 'history 2', 'history 3', 'history 4', 'history 5', 'history 6', 'history 7', 'hours', 'humor 0', 'humor 1', 'humor 2', 'humor 3', 'items', 'literature 0', 'literature 1', 'literature 10', 'literature 11', 'literature 12', 'literature 13', 'literature 14', 'literature 15', 'literature 16', 'literature 17', 'literature 18', 'literature 19', 'literature 2', 'literature 20', 'literature 21', 'literature 22', 'literature 23', 'literature 24', 'literature 25', 'literature 26', 'literature 27', 'literature 28', 'literature 3', 'literature 4', 'literature 5', 'literature 6', 'literature 7', 'literature 8', 'literature 9', 'money 0', 'money 1', 'money 10', 'money 11', 'money 12', 'money 13', 'money 14', 'money 15', 'money 2', 'money 3', 'money 4', 'money 5', 'money 6', 'money 7', 'money 8', 'money 9', 'movies 0', 'movies 1', 'movies 10', 'movies 11', 'movies 12', 'movies 13', 'movies 14', 'movies 15', 'movies 16', 'movies 17', 'movies 18', 'movies 19', 'movies 2', 'movies 20', 'movies 21', 'movies 22', 'movies 3', 'movies 4', 'movies 5', 'movies 6', 'movies 7', 'movies 8', 'movies 9', 'opentoday', 'payments', 'politics 0', 'politics 1', 'politics 10', 'politics 11', 'politics 12', 'politics 2', 'politics 3', 'politics 4', 'politics 5', 'politics 6', 'politics 7', 'politics 8', 'politics 9', 'profile 0', 'profile 1', 'profile 10', 'profile 11', 'profile 12', 'profile 2', 'profile 3', 'profile 4', 'profile 5', 'profile 6', 'profile 7', 'profile 8', 'profile 9', 'psychology 0', 'psychology 1', 'psychology 10', 'psychology 11', 'psychology 12', 'psychology 13', 'psychology 14', 'psychology 15', 'psychology 16', 'psychology 17', 'psychology 18', 'psychology 19', 'psychology 2', 'psychology 20', 'psychology 21', 'psychology 22', 'psychology 23', 'psychology 24', 'psychology 25', 'psychology 26', 'psychology 27', 'psychology 28', 'psychology 29', 'psychology 3', 'psychology 30', 'psychology 31', 'psychology 32', 'psychology 33', 'psychology 34', 'psychology 35', 'psychology 36', 'psychology 37', 'psychology 38', 'psychology 39', 'psychology 4', 'psychology 40', 'psychology 41', 'psychology 42', 'psychology 43', 'psychology 44', 'psychology 45', 'psychology 46', 'psychology 47', 'psychology 48', 'psychology 49', 'psychology 5', 'psychology 50', 'psychology 51', 'psychology 52', 'psychology 53', 'psychology 54', 'psychology 55', 'psychology 56', 'psychology 57', 'psychology 58', 'psychology 59', 'psychology 6', 'psychology 60', 'psychology 61', 'psychology 62', 'psychology 63', 'psychology 64', 'psychology 65', 'psychology 66', 'psychology 67', 'psychology 68', 'psychology 69', 'psychology 7', 'psychology 70', 'psychology 71', 'psychology 72', 'psychology 73', 'psychology 74', 'psychology 75', 'psychology 76', 'psychology 77', 'psychology 78', 'psychology 79', 'psychology 8', 'psychology 80', 'psychology 81', 'psychology 82', 'psychology 83', 'psychology 84', 'psychology 85', 'psychology 86', 'psychology 87', 'psychology 88', 'psychology 9', 'science 0', 'science 1', 'science 10', 'science 11', 'science 12', 'science 13', 'science 14', 'science 15', 'science 16', 'science 17', 'science 18', 'science 19', 'science 2', 'science 20', 'science 21', 'science 22', 'science 3', 'science 4', 'science 5', 'science 6', 'science 7', 'science 8', 'science 9', 'sports 0', 'sports 1', 'sports 10', 'sports 11', 'sports 12', 'sports 13', 'sports 14', 'sports 15', 'sports 16', 'sports 17', 'sports 18', 'sports 19', 'sports 2', 'sports 3', 'sports 4', 'sports 5', 'sports 6', 'sports 7', 'sports 8', 'sports 9', 'thanks', 'trivia 0', 'trivia 1', 'trivia 2', 'trivia 3', 'trivia 4', 'trivia 5', 'trivia 6', 'trivia 7', 'trivia 8', 'trivia 9']\n",
      "539 unique stemmed words: [\"'s\", ',', '1', '1990', '20th-centuri', '23', '37th', '9000', 'a', 'abl', 'about', 'absorb', 'accept', 'act', 'addict', 'afraid', 'after', 'age', 'ai', 'alcohol', 'algorithm', 'aliv', 'allow', 'am', 'american', 'amus', 'an', 'and', 'android', 'angri', 'ani', 'annoy', 'anthem', 'anthoni', 'anybodi', 'anyon', 'appoint', 'are', 'arrog', 'arthur', 'artifici', 'asham', 'asimov', 'ask', 'ass', 'assassin', 'astronom', 'at', 'avogadro', 'avoid', 'axi', 'bacteriolog', 'bad', 'baggin', 'basebal', 'basketbal', 'be', 'been', 'bend', 'best', 'better', 'between', 'bilbo', 'bioinformat', 'blade', 'bodi', 'book', 'bore', 'boss', 'bot', 'boyfriend', 'bradburi', 'brag', 'brain', 'breath', 'brother', 'busi', 'by', 'bye', 'c', 'ca', 'cake', 'call', 'can', 'capabl', 'capit', 'carcinogen', 'card', 'cash', 'caulfield', 'caus', 'celtic', 'charg', 'charlatan', 'chat', 'chatterbox', 'chaucer', 'cheat', 'chemistri', 'child', 'children', 'civil', 'clark', 'clinic', 'clone', 'club', 'cold', 'commun', 'communist', 'compani', 'competit', 'complex', 'complic', 'comput', 'concern', 'context', 'contin', 'continent', 'control', 'corrupt', 'could', 'countri', 'coward', 'cramp', 'crazi', 'credit', 'cricket', 'critic', 'cruel', 'crystallographi', 'cytolog', 'damag', 'data', 'davi', 'day', 'dead', 'deceit', 'deliveri', 'derang', 'determin', 'did', 'die', 'dire', 'dirti', 'discuss', 'diseas', 'disgust', 'dishonest', 'disk', 'do', 'doe', 'dollar', 'dolphin', 'dr.', 'dream', 'drink', 'drunk', 'dull', 'each', 'earn', 'earth', 'eat', 'econom', 'electr', 'embarrass', 'emot', 'energi', 'engin', 'entiti', 'ever', 'experi', 'experienc', 'explain', 'f.', 'faki', 'far', 'father', 'favorit', 'fear', 'feel', 'felt', 'fight', 'first', 'food', 'footbal', 'for', 'forget', 'frank', 'frankenstein', 'frenet', 'friend', 'from', 'fun', 'funni', 'galaxi', 'geoffrey', 'get', 'gibson', 'glad', 'go', 'god', 'godzilla', 'good', 'goodby', 'gossip', 'govern', 'governor', 'gravit', 'greatest', 'greenpeac', 'greet', 'guilti', 'gun', 'h2o', 'ha', 'hal', 'hal9000', 'happi', 'harder', 'hardwar', 'hate', 'have', 'health', 'heard', 'hello', 'help', 'herbert', 'here', 'hey', 'hi', 'hide', 'histori', 'hobbi', 'hobbit', 'holden', 'homer', 'honest', 'hope', 'hopeless', 'hour', 'how', 'hubbl', 'humour', 'husband', 'i', 'ichthyolog', 'idea', 'idiot', 'if', 'illuminati', 'illuminatu', 'immatur', 'immort', 'impeach', 'in', 'indecis', 'insecur', 'insensit', 'insid', 'interest', 'into', 'intox', 'invent', 'invest', 'irrever', 'is', 'it', 'item', 'jacob', 'jaw', 'jealou', 'john', 'joke', 'jule', 'keep', 'kennedi', 'kind', 'kisser', 'know', 'languag', 'later', 'laugh', 'launch', 'law', 'lem', 'leo', 'let', 'lie', 'life', 'lightbulb', 'like', 'linguist', 'live', 'locat', 'lone', 'long', 'longfellow', 'look', 'loosen', 'lord', 'loser', 'lot', 'love', 'low', 'lunat', 'maco', 'mad', 'made', 'major', 'make', 'malfunct', 'man', 'mani', 'market', 'mastercard', 'mate', 'matrix', 'me', 'mean', 'meet', 'messi', 'microprocessor', 'milki', 'mind', 'money', 'mood', 'moon', 'more', 'morn', 'motormouth', 'mount', 'move', 'mr.', 'mrs.', 'ms.', 'much', 'mumbl', 'my', \"n't\", 'name', 'nation', 'nearbi', 'nearest', 'need', 'nervou', 'never', 'news', 'nice', 'no', 'not', 'now', 'number', 'of', 'offend', 'on', 'onli', 'open', 'oper', 'or', 'orbit', 'orient', 'over', 'owner', 'paid', 'pain', 'paranoid', 'parent', 'part', 'pay', 'paypal', 'pedant', 'pick', 'pier', 'plato', 'play', 'player', 'pleasur', 'pothead', 'presid', 'pretenti', 'pro', 'product', 'program', 'programm', 'project', 'psychiatrist', 'psycho', 'psychopath', 'publicli', 'put', 'que', 'queen', 'question', 'quitter', 'race', 'ratchet', 'rate', 'ray', 'read', 'relationship', 'resist', 'revis', 'ride', 'ring', 'rival', 'robot', 'rotat', 'runner', 's', 'sad', 'safe', 'same', 'sapient', 'satellit', 'save', 'saw', 'say', 'scare', 'schizophren', 'sea', 'seab', 'see', 'seen', 'self', 'sell', 'sens', 'sentient', 'serious', 'shape', 'sheep', 'shelf', 'ship', 'shoe', 'shortag', 'should', 'similar', 'sincer', 'size', 'slick', 'smart', 'smith', 'soccer', 'solari', 'some', 'someth', 'sonar', 'sound', 'space', 'spaceflight', 'spider', 'spiderman', 'spin', 'spous', 'stand', 'stanislaw', 'state', 'steam', 'stock', 'stupid', 'subject', 'sun', 'super', 'supremaci', 'surviv', 'system', 'take', 'teacher', 'teknolust', 'telescop', 'tell', 'than', 'thank', 'that', 'the', 'there', 'thermodynam', 'thi', 'think', 'tilt', 'to', 'today', 'togeth', 'tolstoy', 'top', 'toy', 'tri', 'true', 'two', 'type', 'ultrasound', 'unaffect', 'unattract', 'uncultur', 'under', 'understand', 'unhappi', 'unit', 'up', 'use', 'venu', 'vern', 'veut', 'vineland', 'volleybal', 'wa', 'walk', 'want', 'war', 'wast', 'wavelength', 'way', 'we', 'what', 'when', 'where', 'whi', 'which', 'who', 'wife', 'will', 'william', 'window', 'wine', 'wish', 'with', 'wonder', 'work', 'worri', 'worst', 'would', 'written', 'wrote', 'xfind', 'year', 'yesterday', 'yoda', 'yolo', 'you', 'your']\n",
      "539 427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.6668\n",
      "Epoch [200/1000], Loss: 0.0216\n",
      "Epoch [300/1000], Loss: 0.0014\n",
      "Epoch [400/1000], Loss: 0.0006\n",
      "Epoch [500/1000], Loss: 0.0001\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 1.3136\n",
      "Epoch [1000/1000], Loss: 0.0000\n",
      "final loss: 0.0000\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk_utils import bag_of_words, tokenize, stem\n",
    "from model import NeuralNet\n",
    "\n",
    "with open('jsons/master_intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # add to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # add to our words list\n",
    "        all_words.extend(w)\n",
    "        # add to xy pair\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# stem and lower each word\n",
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)\n",
    "\n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_save.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'intents.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-54bd5b698bbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'intents.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mintents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'intents.json'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "print(model.eval)\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(model,r'chat_model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print (\"Starting script1\")\n",
    "os.system(\"python yaml2json.py\")\n",
    "print (\"script1 ended\")\n",
    "print (\"Starting script2\")\n",
    "os.system(\"python convert.py\")\n",
    "print (\"script2 ended\")\n",
    "print (\"Starting script3\")\n",
    "os.system(\"python merge.py\")\n",
    "print (\"script3 ended\")\n",
    "print (\"Starting script4\")\n",
    "os.system(\"python train.py\")\n",
    "print (\"script4 ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat_API.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('jsons/sample_intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Thor\"\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "# while True:\n",
    "#     # sentence = \"do you use credit cards?\"\n",
    "#     sentence = input(\"You: \")\n",
    "#     if sentence == \"quit\":\n",
    "#         break\n",
    "def brain(sentence):\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                # print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "                response = random.choice(intent['responses'])\n",
    "                return response\n",
    "\n",
    "    else:\n",
    "        # print(f\"{bot_name}: I do not understand...\")\n",
    "        response = \"I do not understand...\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template, jsonify, request,redirect, url_for\n",
    "from flask_cors import CORS\n",
    "import chat_API\n",
    "import pywhatkit\n",
    "import pickle\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "# run_with_ngrok(app)               #this is needed to use ngrok for port forwarding\n",
    "database={'subho':'hero','soham':'chomu','yash':'pagla','tublu':'7415'}\n",
    "\n",
    "@app.route('/predict',methods=['GET','POST'])\n",
    "def index():\n",
    "    user_input = request.args.get('user_input')\n",
    "    user_input = user_input.lower()\n",
    "    print(user_input)\n",
    "    if \"search youtube\" in user_input:\n",
    "        user_input = user_input.replace('search youtube', '')  \n",
    "        pywhatkit.playonyt(user_input)\n",
    "        return jsonify({'user_input':str(\"searched youtube\"+str(user_input))})\n",
    "    elif \"search\" in user_input:\n",
    "        user_input = user_input.replace('search', '')  \n",
    "        pywhatkit.search(user_input) \n",
    "        return jsonify({'user_input':str(\"searched\"+str(user_input))})\n",
    "    else:\n",
    "        return jsonify({'user_input':str(chat_API.brain(user_input))})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "You: hi\n",
      "Thor: Hi there, how can I help?\n",
      "Thor: i am mighty THOR\n",
      "You: tell me a joke\n",
      "Thor: what do you get when you cross a cow and a lemon?\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from model import NeuralNet\n",
    "from nltk_utils import bag_of_words, tokenize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('jsons/master_intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Thor\"\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Apr/2021 14:51:33] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Apr/2021 14:51:33] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://ab9af3844279.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,render_template, jsonify, request,redirect, url_for\n",
    "from flask_cors import CORS\n",
    "import chat_API\n",
    "import pywhatkit\n",
    "import pickle\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "run_with_ngrok(app)               #this is needed to use ngrok for port forwarding\n",
    "database={'subho':'hero','soham':'chomu','yash':'pagla','tublu':'7415'}\n",
    "\n",
    "# @app.route('/predict',methods=['POST'])\n",
    "# def index():\n",
    "#     user_input = request.args.get('user_input')\n",
    "#     return jsonify({'user_input':str(chat_API.brain(user_input))})\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"login.html\")\n",
    "\n",
    "@app.route('/form_login',methods=['POST','GET'])\n",
    "def login():\n",
    "    name1=request.form['username']\n",
    "    pwd=request.form['password']\n",
    "    if name1 not in database:\n",
    "\t    return render_template('login.html',info='Invalid User')\n",
    "    else:\n",
    "        if database[name1]!=pwd:\n",
    "            return render_template('login.html',info='Invalid Password')\n",
    "        else:\n",
    "\t        return render_template('index.html',name=name1)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('user_input')\n",
    "    userText = userText.lower()\n",
    "    if \"search youtube\" in userText:\n",
    "        userText = userText.replace('search youtube', '')  \n",
    "        pywhatkit.playonyt(userText)\n",
    "        return \"searched youtube\"+str(userText)\n",
    "    elif \"search\" in userText:\n",
    "        userText = userText.replace('search', '')  \n",
    "        pywhatkit.search(userText) \n",
    "        return \"searched\"+str(userText)\n",
    "    elif \"whatsapp\" in userText:\n",
    "        return \"I am trying\" + \"I cannot open whatsapp\"\n",
    "    elif \"what is your name\" in userText:\n",
    "        return \"My name is Chatterbot\"\n",
    "    return str(chat_API.brain(userText))\n",
    "    \n",
    "@app.route('/logout')\n",
    "def logout():\n",
    "    return redirect(url_for('home'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
